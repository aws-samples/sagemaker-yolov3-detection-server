{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an image detection server with gluoncv\n",
    "In this notebook, we deploy a pre-trained image detection model\n",
    " 1. We first import a model from the gluon model zoo locally on the notebook, that we then compress and send to S3\n",
    " 1. We then use the SageMaker MXNet Serving feature to deploy the model to a real-time managed endpoint. It uses the model artifact that we previously loaded to S3.\n",
    " 1. We query the endpoint and visualize detection results\n",
    "\n",
    "\n",
    "* **More on `gluon` and `gluoncv`:**\n",
    " * [gluon](https://mxnet.incubator.apache.org/api/python/docs/api/gluon/index.html)** is the imperative python front-end of the Apache MXNet deep learning framework. Gluon notably features specialized toolkits helping reproducing state-of-the-art architectures: [gluon-cv](https://gluon-cv.mxnet.io/), [gluon-nlp](https://gluon-nlp.mxnet.io/), [gluon-ts](https://gluon-ts.mxnet.io/). Gluon also features a number of excellent end-to-end tutorial mixing science with code such as [D2L.ai](https://classic.d2l.ai/) and [The Straight Dope](https://gluon.mxnet.io/)\n",
    " * [gluoncv](https://gluon-cv.mxnet.io/contents.html) is an efficient computer vision toolkit written on top of `gluon` and MXNet aiming to make state-of-the-art vision research reproducible. \n",
    "\n",
    "* This specific demo has been developed on the `conda_mxnet_p36` kernel of a SageMaker `ml.c5.2xlarge` Notebook instance\n",
    "\n",
    "**This sample is provided for demonstration purposes, make sure to conduct appropriate testing if derivating this code for your own use-cases!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import subprocess as sb\n",
    "import tarfile\n",
    "\n",
    "import boto3\n",
    "import gluoncv\n",
    "from gluoncv import model_zoo, data, utils\n",
    "from matplotlib import pyplot as plt\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, image, nd\n",
    "\n",
    "# import SageMaker tools\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.mxnet.model import MXNetModel\n",
    "\n",
    "sm_sess = sagemaker.Session()\n",
    "\n",
    "s3_bucket = sm_sess.default_bucket()  # We use this bucket to store model weights - don't hesitate to change.\n",
    "print(f'using bucket {s3_bucket}')\n",
    "\n",
    "sm_role = sm_sess.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a pre-trained detection model, locally\n",
    "gluon model zoo https://mxnet.incubator.apache.org/api/python/gluon/model_zoo.html contains a variety of models.\n",
    "In this demo we use a YoloV3 detection model (Redmon et Farhadi). More about YoloV3:\n",
    "* paper https://pjreddie.com/media/files/papers/YOLOv3.pdf\n",
    "* Website https://pjreddie.com/darknet/yolo/\n",
    "\n",
    "Gluoncv model zoo contains a number of architectures with different performances in terms of speed and accuracy. If you are looking for speed or accuracy, don't hesitate to change the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'yolo3_darknet53_coco'\n",
    "net = model_zoo.get_model(model_name, pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we downloaded above is trained on the COCO dataset and can detect 80 classes. In this demo, we restrict the model to detect only specific classes of interest.\n",
    "This idea is derived from the official gluoncv tutorial: https://gluon-cv.mxnet.io/build/examples_detection/skip_fintune.html\n",
    "\n",
    "\n",
    "COCO contains the following classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('coco classes: ', net.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# in this demo we reset the detector to the \"person\" class\n",
    "\n",
    "classes = ['person']\n",
    "\n",
    "net.reset_class(classes=classes, reuse_weights=classes)\n",
    "print('new classes: ', net.classes)\n",
    "\n",
    "net.hybridize()  # hybridize to optimize computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather couple public images\n",
    "images = ['https://cdn.pixabay.com/photo/2017/06/09/05/17/person-2385787_960_720.jpg',\n",
    "          'https://cdn.pixabay.com/photo/2019/11/04/01/11/cellular-4599956_960_720.jpg',\n",
    "          #'https://cdn.pixabay.com/photo/2016/03/09/09/22/workplace-1245776_1280.jpg',\n",
    "          'https://cdn.pixabay.com/photo/2017/07/31/21/04/people-2561053_1280.jpg',\n",
    "          'https://cdn.pixabay.com/photo/2016/08/01/20/13/girl-1561989_1280.jpg',\n",
    "          'https://cdn.pixabay.com/photo/2015/01/08/18/29/entrepreneur-593358_1280.jpg',\n",
    "          #'https://cdn.pixabay.com/photo/2014/05/03/00/50/flower-child-336658_1280.jpg',\n",
    "          'https://cdn.pixabay.com/photo/2014/07/31/23/49/guitarist-407212_1280.jpg',\n",
    "          'https://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Cyprus_national_football_team_2012.jpg/1600px-Cyprus_national_football_team_2012.jpg',\n",
    "          'https://pixnio.com/free-images/2019/01/13/2019-01-13-09-46-22-1200x900.jpg',\n",
    "          'https://cdn.pixabay.com/photo/2018/01/24/19/49/people-3104635_1280.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the images locally\n",
    "\n",
    "pic_folder = 'gluoncv-detector-pics'\n",
    "\n",
    "sb.call(['mkdir', pic_folder])\n",
    "\n",
    "shutil.rmtree(os.path.join(os.getcwd(), pic_folder))\n",
    "\n",
    "for p in images:\n",
    "    sb.call(['wget', p, '-P', pic_folder])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gluoncv` comes with built-in pre-processing logic for popular detectors, including YoloV3:\n",
    "\n",
    "https://gluon-cv.mxnet.io/_modules/gluoncv/data/transforms/presets/yolo.html\n",
    "\n",
    "https://gluon-cv.mxnet.io/build/examples_detection/demo_yolo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picnames = os.listdir(pic_folder)\n",
    "\n",
    "n_pics = len(picnames)\n",
    "n_cols = 3\n",
    "n_rows = math.ceil(n_pics / n_cols)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15,15))\n",
    "[ax.axis(\"off\") for ax_dim in axes for ax in ax_dim]\n",
    "for i, pic in enumerate(picnames):\n",
    "    curr_col = i % n_cols\n",
    "    curr_row = i // n_cols\n",
    "    # download and pre-process image\n",
    "    print(pic)\n",
    "    im_array =  image.imread(os.path.join(pic_folder, pic))\n",
    "    x, orig_img = data.transforms.presets.yolo.transform_test(im_array)\n",
    "    \n",
    "    # forward pass and display\n",
    "    box_ids, scores, bboxes = net(x)\n",
    "    ax = utils.viz.plot_bbox(orig_img, bboxes[0], scores[0], box_ids[0], class_names=classes, thresh=0.9, ax=axes[curr_row, curr_col])\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(pic, pad=15)\n",
    "fig.tight_layout()    \n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the detection server\n",
    " 1. We first need to **send the model to S3**, as we will provide the S3 model path to Amazon SageMaker endpoint creation API\n",
    " 1. We create a **serving script** containing model deserialization code and inference logic\n",
    " 1. We **deploy the endpoint** with a SageMaker SDK call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save local model, compress and send to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the full model (both weights and graph)\n",
    "net.export(model_name, epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compress\n",
    "packname = 'model.tar.gz'\n",
    "tar = tarfile.open(packname, 'w:gz')\n",
    "tar.add('{}-symbol.json'.format(model_name))\n",
    "tar.add('{}-0000.params'.format(model_name))\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to s3\n",
    "s3_client = boto3.client('s3')\n",
    "s3key = 'detection-artifact'\n",
    "s3_client.upload_file(packname, s3_bucket, s3key + '/' + packname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the detection model in the SageMaker MXNet specification\n",
    "https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/mxnet/README.rst#deploying-mxnet-models\n",
    "\n",
    "Amazon SageMaker provides serving container for Sklearn, TensorFlow, PyTorch and Apache MXNet. This is convenient, because we don't have to write web server code: the server is already written, in the case of MXNet it is Multi Model Server ([MMS](https://github.com/awslabs/multi-model-server), also used to server PyTorch in SageMaker) . We just have to provide model deserialization code and serving logic.\n",
    "\n",
    "The SageMaker MXNet model server breaks request handling into three steps. Each step involves invoking a python function, with information about the request and the return-value from the previous function in the chain:\n",
    "\n",
    "* input processing, with `input_fn(request_body, request_content_type, model)`\n",
    "* prediction, with `predict_fn(input_object, model)`\n",
    "* output processing, with `output_fn(prediction, content_type)`\n",
    "\n",
    "The full serving specification is documented here https://sagemaker.readthedocs.io/en/stable/using_mxnet.html#deploy-mxnet-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our specific example we don't write an `output_fn`, because `predict_fn` outputs an NDArray that can be handled to CSV or JSON by the default `output_fn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will store the script in the 'repo' directory\n",
    "! mkdir repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a requirements.txt to add an extra dependency to the SageMaker MXNet container (https://github.com/aws/sagemaker-mxnet-serving-container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile repo/requirements.txt\n",
    "gluoncv==0.10.4.post4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile repo/detection_server.py\n",
    "\n",
    "import argparse\n",
    "import ast\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from gluoncv import model_zoo, data, utils\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon\n",
    "\n",
    "\n",
    "def get_ctx():\n",
    "    \"function to get machine hardware context\"\n",
    "    try:\n",
    "        _ = mx.nd.array([0], ctx=mx.gpu())\n",
    "        ctx = mx.gpu()\n",
    "    except:\n",
    "        try:\n",
    "            _ = mx.nd.array([0], ctx=mx.eia())\n",
    "            ctx = mx.eia()\n",
    "        except: \n",
    "            ctx = mx.cpu()\n",
    "    return ctx\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Load the gluon model. Called once when hosting service starts.\n",
    "    :param: model_dir The directory where model files are stored.\n",
    "    :return: a model (in this case a Gluon network)\n",
    "    \n",
    "    assumes that the parameters artifact is {model_name}.params\n",
    "    \"\"\"\n",
    "    \n",
    "    ctx = get_ctx()\n",
    "    logging.info('Using ctx {}'.format(ctx))\n",
    "    logging.info('Dir content {}'.format(os.listdir()))\n",
    "    \n",
    "    # instantiate net and reset to classes of interest\n",
    "    net = gluon.nn.SymbolBlock.imports(\n",
    "        symbol_file=[f for f in os.listdir() if f.endswith('json')][0],\n",
    "        input_names=['data'],\n",
    "        param_file=[f for f in os.listdir() if f.endswith('params')][0],\n",
    "        ctx=ctx)\n",
    "    \n",
    "    return net\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"prepares the input\"\"\"\n",
    "        \n",
    "    im_array = mx.image.imdecode(request_body)\n",
    "    \n",
    "    # Run YOLO pre-processing on CPU\n",
    "    x, _ = data.transforms.presets.yolo.transform_test(im_array)\n",
    "    logging.info('input_fn returns NDArray of shape ' + str(im_array.shape))\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def predict_fn(input_object, model):\n",
    "    \"\"\"function used for prediction\"\"\"\n",
    "    \n",
    "    ctx = get_ctx()\n",
    "    logging.info('Using ctx {}'.format(ctx))\n",
    "    \n",
    "    # forward pass and display\n",
    "    box_ids, scores, bboxes = model(input_object.as_in_context(ctx))\n",
    "    \n",
    "    return nd.concat(box_ids, scores, bboxes, dim=2)  # return a single tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model and deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MXNetModel(\n",
    "    model_data='s3://{}/{}/{}'.format(s3_bucket, s3key, packname),\n",
    "    role=sm_role,\n",
    "    py_version='py37',\n",
    "    entry_point='detection_server.py',\n",
    "    source_dir='repo',\n",
    "    framework_version='1.8.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We deploy to an ml.g4dn.xlarge instance featuring a T4 NVIDIA GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_key = ((model_name + '-detection').replace('_', '-').replace('.', '') + '-' \n",
    "                + datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import sagemaker.serializers\n",
    "import sagemaker.deserializers\n",
    "\n",
    "\n",
    "# this may take 5 to 10min\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.g4dn.xlarge',\n",
    "    endpoint_name=endpoint_key,\n",
    "    serializer=sagemaker.serializers.JSONSerializer,\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.deserializer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit requests to the detection server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor = sagemaker.predictor.Predictor(endpoint_name=endpoint_key, serializer=sagemaker.serializers.JSONSerializer, deserializer=sagemaker.deserializers.JSONDeserializer)\n",
    "predictor = sagemaker.predictor.Predictor(endpoint_name=endpoint_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(pic, predictor):\n",
    "    \"\"\"elementary function to send a picture to a predictor\"\"\"\n",
    "    \n",
    "    with open(pic, 'rb') as f:\n",
    "        image = f.read()\n",
    "    #print(image)\n",
    "    tensor = nd.array(json.loads(predictor.predict(image)))\n",
    "    box_ids, scores, bboxes = tensor[:,:,0], tensor[:,:,1], tensor[:,:,2:]\n",
    "    return box_ids, scores, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# test\n",
    "\n",
    "pic = 'girl-1561989_1280.jpg'\n",
    "\n",
    "box_ids, scores, bboxes = detect(os.path.join(pic_folder, pic), predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for local viz we need to resize local pic to the server-side resize\n",
    "_, orig_img = data.transforms.presets.yolo.load_test(os.path.join(pic_folder, pic))\n",
    "utils.viz.plot_bbox(orig_img, bboxes[0], scores[0], box_ids[0], class_names=classes, thresh=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsuprisingly this works as well! The main difference with the first demo done locally at the top of the notebook is that we now have a permanently up, remote ML model server reachable with API, along with server and hardware monitoring available in Cloudwatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't forget to delete the endpoint after the demo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_sess.delete_endpoint(endpoint_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
